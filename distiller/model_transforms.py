import torch
import torch.nn as nn
from collections import OrderedDict

import distiller
import distiller.modules
from distiller.quantization.sim_bn_fold import SimulatedFoldedBatchNorm
import logging
msglogger = logging.getLogger()


def _fuse_sequence(sequence, named_modules, fuse_fn):
    names = [m.distiller_name for m in sequence]
    msglogger.debug('Fusing sequence {}'.format(names))

    # Call fusing function
    fused_module = fuse_fn(sequence)
    if fused_module is None:
        msglogger.debug('Sequence {} was not fused'.format(names))
        return

    # Leave a 'mark' in the fused module, indicating which modules were fused. This can come in handy
    # post-fusing, since the identity nodes don't show up in SummrayGraph (they're optimized away).
    setattr(sequence[0], 'fused_modules', names[1:])

    # Replace the first module in the sequence with the fused module
    def split_name(name):
        if '.' in name:
            return name.rsplit('.', 1)
        else:
            return '', name
    container_name, root_module = split_name(names[0])
    container = named_modules[container_name]
    setattr(container, root_module, fused_module)

    # Replace the rest of the models in the sequence with identity ops
    for container_name, sub_module_name in map(lambda name: split_name(name), names[1:]):
        container = named_modules[container_name]
        setattr(container, sub_module_name, nn.Identity())


def fuse_modules(model, dummy_input, types_sequence, fuse_fn):
    """
    Scans the module for sequences of modules of the specified types and "fuses" them. As an example, consider the
    following sequence of 3 modules: 'm_1' --> 'm_2' --> 'm_3'. Assuming they match the specified sequence of types,
    they will be fused such that the "fused" module replaces 'm_1', and 'm_2' and 'm_3' are replaced with identity
    operations.

    For a sequence of modules to be fused, it must not contain splits. That is - no module in the sequence can
    have more than a single output. For example, consider the following sequence:

    m_1 --> m_2 --> m_3
        |
        |
        --> m_4

    Even if m_1, m_2 and m_3 match the types sequence, they can't be fused because m_1's output also goes to m_4.

    The fused module is generated by the user specified function 'fuse_fn'.

    To infer the order of modules it is required to perform a forward pass on the model. Hence the need to pass the
    expected input shape.

    Args:
        model (nn.Module): Model instance on which the transformation is performed
        dummy_input (torch.Tensor or tuple): Dummy input to the model
        types_sequence (list or tuple): Sequence of module types. Each item in the sequence may itself be a
          list / tuple. For example - to fuse all possible convolution types with ReLU, pass:
          [[nn.Conv1d, nn.Conv2d, nn.Conv3d], nn.ReLU]
        fuse_fn (function): Function that takes a list of models to be fused, and returns a single fused module.
          If the sequence cannot be fused, this function should return None
    """
    distiller.assign_layer_fq_names(model)
    sg = distiller.SummaryGraph(model, dummy_input)
    adjacency_map = sg.adjacency_map(dedicated_modules_only=False)
    named_modules = OrderedDict(model.named_modules())
    in_sequence_idx = 0
    curr_sequence = []

    for node_name, adj_entry in adjacency_map.items():
        module = named_modules.get(node_name, None)
        if module and isinstance(module, types_sequence[in_sequence_idx]):
            curr_sequence.append(module)
            in_sequence_idx += 1
            if in_sequence_idx == len(types_sequence):
                _fuse_sequence(curr_sequence, named_modules, fuse_fn)
                in_sequence_idx = 0
                curr_sequence = []
            elif len(adj_entry.successors) > 1:
                msglogger.debug(node_name + " is connected to multiple outputs, not fuse-able")
                in_sequence_idx = 0
                curr_sequence = []
        else:
            in_sequence_idx = 0
            curr_sequence = []
    return model

